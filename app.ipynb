{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/bigscience-workshop/petals\n",
    "%pip install --upgrade langchain"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-01T17:49:27.218891516Z"
    }
   },
   "id": "5193396351fc1ae1"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nov 02 01:00:54.155 [\u001B[1m\u001B[34mINFO\u001B[0m] Make sure you follow the LLaMA's terms of use: https://bit.ly/llama2-license for LLaMA 2, https://bit.ly/llama-license for LLaMA 1\n",
      "Nov 02 01:00:54.155 [\u001B[1m\u001B[34mINFO\u001B[0m] Using DHT prefix: StableBeluga2-hf\n",
      "torch_shm_manager: error while loading shared libraries: libcupti.so.12: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "no response from torch_shm_manager at \"/home/biscuitbobby/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/bin/torch_shm_manager\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpetals-team/StableBeluga2\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      8\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name, use_fast\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, add_bos_token\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m----> 9\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoDistributedModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mcuda()  \u001B[38;5;66;03m# Move the model to GPU if available\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(model)\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/petals/utils/auto_config.py:78\u001B[0m, in \u001B[0;36mDefaultRevisionMixin.from_pretrained\u001B[0;34m(cls, model_name_or_path, revision, *args, **kwargs)\u001B[0m\n\u001B[1;32m     76\u001B[0m     revision \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mDEFAULT_REVISIONS[model_name_or_path]\n\u001B[1;32m     77\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, revision \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrevision\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 78\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/petals/utils/auto_config.py:51\u001B[0m, in \u001B[0;36m_AutoDistributedBase.from_pretrained\u001B[0;34m(cls, model_name_or_path, *args, **kwargs)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m proper_cls \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPetals does not have \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for model type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39mmodel_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 51\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mproper_cls\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/petals/client/from_pretrained.py:31\u001B[0m, in \u001B[0;36mFromPretrainedMixin.from_pretrained\u001B[0;34m(cls, model_name_or_path, low_cpu_mem_usage, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m     low_cpu_mem_usage \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ignore_keys(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_keys_to_ignore_on_load_unexpected):\n\u001B[0;32m---> 31\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3085\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3082\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_check_and_enable_flash_attn_2(config, torch_dtype\u001B[38;5;241m=\u001B[39mtorch_dtype, device_map\u001B[38;5;241m=\u001B[39mdevice_map)\n\u001B[1;32m   3084\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ContextManagers(init_contexts):\n\u001B[0;32m-> 3085\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3087\u001B[0m \u001B[38;5;66;03m# Check first if we are `from_pt`\u001B[39;00m\n\u001B[1;32m   3088\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_keep_in_fp32_modules:\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/petals/models/llama/model.py:132\u001B[0m, in \u001B[0;36mDistributedLlamaForCausalLM.__init__\u001B[0;34m(self, config)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, config: DistributedLlamaConfig):\n\u001B[1;32m    131\u001B[0m     LlamaPreTrainedModel\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, config)\n\u001B[0;32m--> 132\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[43mDistributedLlamaModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpretraining_tp \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mpretraining_tp\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab_size \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mvocab_size\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/petals/models/llama/model.py:34\u001B[0m, in \u001B[0;36mDistributedLlamaModel.__init__\u001B[0;34m(self, config, dht)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     32\u001B[0m config\u001B[38;5;241m.\u001B[39mnum_hidden_layers \u001B[38;5;241m=\u001B[39m n_layer\n\u001B[0;32m---> 34\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers \u001B[38;5;241m=\u001B[39m \u001B[43mRemoteSequential\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdht\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdht\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequires_grad_(\u001B[38;5;28;01mFalse\u001B[39;00m)  \u001B[38;5;66;03m# Forbid accumulate grads for embeddings and layernorm\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit_prompts(config)\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/petals/client/remote_sequential.py:47\u001B[0m, in \u001B[0;36mRemoteSequential.__init__\u001B[0;34m(self, config, sequence_manager, dht, start_block, end_block, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m         end_block \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers\n\u001B[1;32m     46\u001B[0m     block_uids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39mdht_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mUID_DELIMITER\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(start_block, end_block))\n\u001B[0;32m---> 47\u001B[0m     sequence_manager \u001B[38;5;241m=\u001B[39m \u001B[43mRemoteSequenceManager\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mblock_uids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdht\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdht\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msequence_manager \u001B[38;5;241m=\u001B[39m sequence_manager\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_active_session \u001B[38;5;241m=\u001B[39m ContextVar(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mactive_session\u001B[39m\u001B[38;5;124m\"\u001B[39m, default\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/petals/client/routing/sequence_manager.py:89\u001B[0m, in \u001B[0;36mRemoteSequenceManager.__init__\u001B[0;34m(self, config, block_uids, dht, state)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m state\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dht \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 89\u001B[0m     dht \u001B[38;5;241m=\u001B[39m \u001B[43mDHT\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[43m        \u001B[49m\u001B[43minitial_peers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minitial_peers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     91\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclient_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     93\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstartup_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdaemon_startup_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstart\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     95\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dht, DHT) \u001B[38;5;129;01mand\u001B[39;00m dht\u001B[38;5;241m.\u001B[39mis_alive(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`dht` must be a running hivemind.DHT instance\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdht \u001B[38;5;241m=\u001B[39m dht\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/hivemind/dht/dht.py:77\u001B[0m, in \u001B[0;36mDHT.__init__\u001B[0;34m(self, initial_peers, start, p2p, daemon, num_workers, record_validators, shutdown_timeout, await_ready, **kwargs)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_pipe, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outer_pipe \u001B[38;5;241m=\u001B[39m mp\u001B[38;5;241m.\u001B[39mPipe(duplex\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshutdown_timeout \u001B[38;5;241m=\u001B[39m shutdown_timeout\n\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ready \u001B[38;5;241m=\u001B[39m \u001B[43mMPFuture\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdaemon \u001B[38;5;241m=\u001B[39m daemon\n\u001B[1;32m     80\u001B[0m \u001B[38;5;66;03m# These values will be fetched from the child process when requested\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/hivemind/utils/mpfuture.py:101\u001B[0m, in \u001B[0;36mMPFuture.__init__\u001B[0;34m(self, use_lock)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_initialize_mpfuture_backend()\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_origin_pid, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_uid \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mgetpid(), uuid\u001B[38;5;241m.\u001B[39muuid4()\u001B[38;5;241m.\u001B[39mint\n\u001B[0;32m--> 101\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shared_state_code \u001B[38;5;241m=\u001B[39m \u001B[43mSharedBytes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnext\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state_cache: Dict[State, State] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    103\u001B[0m \u001B[38;5;66;03m# mapping from global to cached local future used that makes updates immediately\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;66;03m# available on setter side; dictionary-based cache works because future can visit any state at most once\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/hivemind/utils/mpfuture.py:60\u001B[0m, in \u001B[0;36mSharedBytes.next\u001B[0;34m(cls)\u001B[0m\n\u001B[1;32m     58\u001B[0m     buffer_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHIVEMIND_SHM_BUFFER_SIZE\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m16\u001B[39m))\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_pid \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mgetpid()\n\u001B[0;32m---> 60\u001B[0m     \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempty\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muint8\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshare_memory_\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_index \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/_tensor.py:684\u001B[0m, in \u001B[0;36mTensor.share_memory_\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    682\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    683\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(Tensor\u001B[38;5;241m.\u001B[39mshare_memory_, (\u001B[38;5;28mself\u001B[39m,), \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 684\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_typed_storage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_share_memory_\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    685\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/storage.py:858\u001B[0m, in \u001B[0;36mTypedStorage._share_memory_\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    857\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_share_memory_\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 858\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_untyped_storage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshare_memory_\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    859\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/storage.py:297\u001B[0m, in \u001B[0;36m_share_memory_lock_protected.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    294\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 297\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    298\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;66;03m# If we acquired the storage lock here and we're done working on it\u001B[39;00m\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;66;03m# we can now release it and free the entry.\u001B[39;00m\n\u001B[1;32m    301\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m to_free \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    302\u001B[0m         \u001B[38;5;66;03m# Ensure that the cdata from the storage didn't change and only\u001B[39;00m\n\u001B[1;32m    303\u001B[0m         \u001B[38;5;66;03m# the data_ptr did.\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/storage.py:326\u001B[0m, in \u001B[0;36mUntypedStorage.share_memory_\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    324\u001B[0m \u001B[38;5;129m@_share_memory_lock_protected\u001B[39m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshare_memory_\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshare_memory_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/storage.py:247\u001B[0m, in \u001B[0;36m_StorageBase.share_memory_\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# CUDA or PrivateUse1 doesn't use POSIX shared memory\u001B[39;00m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m get_sharing_strategy() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfile_system\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 247\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_share_filename_cpu_\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_share_fd_cpu_()\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/storage.py:297\u001B[0m, in \u001B[0;36m_share_memory_lock_protected.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    294\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 297\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    298\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;66;03m# If we acquired the storage lock here and we're done working on it\u001B[39;00m\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;66;03m# we can now release it and free the entry.\u001B[39;00m\n\u001B[1;32m    301\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m to_free \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    302\u001B[0m         \u001B[38;5;66;03m# Ensure that the cdata from the storage didn't change and only\u001B[39;00m\n\u001B[1;32m    303\u001B[0m         \u001B[38;5;66;03m# the data_ptr did.\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/storage.py:334\u001B[0m, in \u001B[0;36mUntypedStorage._share_filename_cpu_\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;129m@_share_memory_lock_protected\u001B[39m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_share_filename_cpu_\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 334\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_share_filename_cpu_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: no response from torch_shm_manager at \"/home/biscuitbobby/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/bin/torch_shm_manager\""
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from transformers import AutoTokenizer\n",
    "from petals import AutoDistributedModelForCausalLM\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "model_name = \"petals-team/StableBeluga2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, add_bos_token=False)\n",
    "model = AutoDistributedModelForCausalLM.from_pretrained(model_name)\n",
    "model = model.cuda()  # Move the model to GPU if available\n",
    "\n",
    "print(model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:30:54.607792284Z",
     "start_time": "2023-11-01T19:30:52.426049935Z"
    }
   },
   "id": "f126c8f3887df09c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from transformers import AutoTokenizer\n",
    "from petals import AutoDistributedModelForCausalLM\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    def __init__(self, model_name: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        object.__setattr__(self, \"tokenizer\", AutoTokenizer.from_pretrained(model_name, use_fast=False, add_bos_token=False))\n",
    "        object.__setattr__(self, \"model\", AutoDistributedModelForCausalLM.from_pretrained(model_name))\n",
    "        self.model.cuda()  # Ensure the model is on GPU\n",
    "\n",
    "    def _call(self, text: str, stop=None, **kwargs) -> str:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "        outputs = self.model.generate(inputs, max_new_tokens=150)\n",
    "        decoded_output = (tokenizer.decode(outputs[0]))\n",
    "        return decoded_output\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"CustomLLM\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:31:47.373966208Z",
     "start_time": "2023-11-01T19:31:47.370147621Z"
    }
   },
   "id": "4b973ed70b09ee02"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nov 02 01:01:49.031 [\u001B[1m\u001B[34mINFO\u001B[0m] Make sure you follow the LLaMA's terms of use: https://bit.ly/llama2-license for LLaMA 2, https://bit.ly/llama-license for LLaMA 1\n",
      "Nov 02 01:01:49.032 [\u001B[1m\u001B[34mINFO\u001B[0m] Using DHT prefix: StableBeluga2-hf\n",
      "torch_shm_manager: error while loading shared libraries: libcupti.so.12: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "no response from torch_shm_manager at \"/home/biscuitbobby/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/bin/torch_shm_manager\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchains\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LLMChain\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Initialize the custom LLM\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m llm \u001B[38;5;241m=\u001B[39m \u001B[43mCustomLLM\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpetals-team/StableBeluga2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Define the prompt template\u001B[39;00m\n\u001B[1;32m      8\u001B[0m template \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124mQuestion: \u001B[39m\u001B[38;5;132;01m{question}\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124mAnswer: Let\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms think step by step.\u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n",
      "Cell \u001B[0;32mIn[7], line 10\u001B[0m, in \u001B[0;36mCustomLLM.__init__\u001B[0;34m(self, model_name, **kwargs)\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mobject\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__setattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizer\u001B[39m\u001B[38;5;124m\"\u001B[39m, AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name, use_fast\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, add_bos_token\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28mobject\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__setattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[43mAutoDistributedModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/petals/utils/auto_config.py:78\u001B[0m, in \u001B[0;36mDefaultRevisionMixin.from_pretrained\u001B[0;34m(cls, model_name_or_path, revision, *args, **kwargs)\u001B[0m\n\u001B[1;32m     76\u001B[0m     revision \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mDEFAULT_REVISIONS[model_name_or_path]\n\u001B[1;32m     77\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, revision \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrevision\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 78\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/petals/utils/auto_config.py:51\u001B[0m, in \u001B[0;36m_AutoDistributedBase.from_pretrained\u001B[0;34m(cls, model_name_or_path, *args, **kwargs)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m proper_cls \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPetals does not have \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for model type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39mmodel_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 51\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mproper_cls\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/petals/client/from_pretrained.py:31\u001B[0m, in \u001B[0;36mFromPretrainedMixin.from_pretrained\u001B[0;34m(cls, model_name_or_path, low_cpu_mem_usage, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m     low_cpu_mem_usage \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ignore_keys(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_keys_to_ignore_on_load_unexpected):\n\u001B[0;32m---> 31\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3085\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3082\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_check_and_enable_flash_attn_2(config, torch_dtype\u001B[38;5;241m=\u001B[39mtorch_dtype, device_map\u001B[38;5;241m=\u001B[39mdevice_map)\n\u001B[1;32m   3084\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ContextManagers(init_contexts):\n\u001B[0;32m-> 3085\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3087\u001B[0m \u001B[38;5;66;03m# Check first if we are `from_pt`\u001B[39;00m\n\u001B[1;32m   3088\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_keep_in_fp32_modules:\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/petals/models/llama/model.py:132\u001B[0m, in \u001B[0;36mDistributedLlamaForCausalLM.__init__\u001B[0;34m(self, config)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, config: DistributedLlamaConfig):\n\u001B[1;32m    131\u001B[0m     LlamaPreTrainedModel\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, config)\n\u001B[0;32m--> 132\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[43mDistributedLlamaModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpretraining_tp \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mpretraining_tp\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab_size \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mvocab_size\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/petals/models/llama/model.py:34\u001B[0m, in \u001B[0;36mDistributedLlamaModel.__init__\u001B[0;34m(self, config, dht)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     32\u001B[0m config\u001B[38;5;241m.\u001B[39mnum_hidden_layers \u001B[38;5;241m=\u001B[39m n_layer\n\u001B[0;32m---> 34\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers \u001B[38;5;241m=\u001B[39m \u001B[43mRemoteSequential\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdht\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdht\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequires_grad_(\u001B[38;5;28;01mFalse\u001B[39;00m)  \u001B[38;5;66;03m# Forbid accumulate grads for embeddings and layernorm\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit_prompts(config)\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/petals/client/remote_sequential.py:47\u001B[0m, in \u001B[0;36mRemoteSequential.__init__\u001B[0;34m(self, config, sequence_manager, dht, start_block, end_block, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m         end_block \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers\n\u001B[1;32m     46\u001B[0m     block_uids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39mdht_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mUID_DELIMITER\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(start_block, end_block))\n\u001B[0;32m---> 47\u001B[0m     sequence_manager \u001B[38;5;241m=\u001B[39m \u001B[43mRemoteSequenceManager\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mblock_uids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdht\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdht\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msequence_manager \u001B[38;5;241m=\u001B[39m sequence_manager\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_active_session \u001B[38;5;241m=\u001B[39m ContextVar(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mactive_session\u001B[39m\u001B[38;5;124m\"\u001B[39m, default\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/petals/client/routing/sequence_manager.py:89\u001B[0m, in \u001B[0;36mRemoteSequenceManager.__init__\u001B[0;34m(self, config, block_uids, dht, state)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m state\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dht \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 89\u001B[0m     dht \u001B[38;5;241m=\u001B[39m \u001B[43mDHT\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[43m        \u001B[49m\u001B[43minitial_peers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minitial_peers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     91\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclient_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     93\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstartup_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdaemon_startup_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstart\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     95\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dht, DHT) \u001B[38;5;129;01mand\u001B[39;00m dht\u001B[38;5;241m.\u001B[39mis_alive(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`dht` must be a running hivemind.DHT instance\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdht \u001B[38;5;241m=\u001B[39m dht\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/hivemind/dht/dht.py:77\u001B[0m, in \u001B[0;36mDHT.__init__\u001B[0;34m(self, initial_peers, start, p2p, daemon, num_workers, record_validators, shutdown_timeout, await_ready, **kwargs)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_pipe, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outer_pipe \u001B[38;5;241m=\u001B[39m mp\u001B[38;5;241m.\u001B[39mPipe(duplex\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshutdown_timeout \u001B[38;5;241m=\u001B[39m shutdown_timeout\n\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ready \u001B[38;5;241m=\u001B[39m \u001B[43mMPFuture\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdaemon \u001B[38;5;241m=\u001B[39m daemon\n\u001B[1;32m     80\u001B[0m \u001B[38;5;66;03m# These values will be fetched from the child process when requested\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/hivemind/utils/mpfuture.py:101\u001B[0m, in \u001B[0;36mMPFuture.__init__\u001B[0;34m(self, use_lock)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_initialize_mpfuture_backend()\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_origin_pid, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_uid \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mgetpid(), uuid\u001B[38;5;241m.\u001B[39muuid4()\u001B[38;5;241m.\u001B[39mint\n\u001B[0;32m--> 101\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shared_state_code \u001B[38;5;241m=\u001B[39m \u001B[43mSharedBytes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnext\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state_cache: Dict[State, State] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    103\u001B[0m \u001B[38;5;66;03m# mapping from global to cached local future used that makes updates immediately\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;66;03m# available on setter side; dictionary-based cache works because future can visit any state at most once\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib/python3.11/site-packages/hivemind/utils/mpfuture.py:60\u001B[0m, in \u001B[0;36mSharedBytes.next\u001B[0;34m(cls)\u001B[0m\n\u001B[1;32m     58\u001B[0m     buffer_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHIVEMIND_SHM_BUFFER_SIZE\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m16\u001B[39m))\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_pid \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mgetpid()\n\u001B[0;32m---> 60\u001B[0m     \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempty\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muint8\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshare_memory_\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_index \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/_tensor.py:684\u001B[0m, in \u001B[0;36mTensor.share_memory_\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    682\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    683\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(Tensor\u001B[38;5;241m.\u001B[39mshare_memory_, (\u001B[38;5;28mself\u001B[39m,), \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 684\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_typed_storage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_share_memory_\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    685\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/storage.py:858\u001B[0m, in \u001B[0;36mTypedStorage._share_memory_\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    857\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_share_memory_\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 858\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_untyped_storage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshare_memory_\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    859\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/storage.py:297\u001B[0m, in \u001B[0;36m_share_memory_lock_protected.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    294\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 297\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    298\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;66;03m# If we acquired the storage lock here and we're done working on it\u001B[39;00m\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;66;03m# we can now release it and free the entry.\u001B[39;00m\n\u001B[1;32m    301\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m to_free \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    302\u001B[0m         \u001B[38;5;66;03m# Ensure that the cdata from the storage didn't change and only\u001B[39;00m\n\u001B[1;32m    303\u001B[0m         \u001B[38;5;66;03m# the data_ptr did.\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/storage.py:326\u001B[0m, in \u001B[0;36mUntypedStorage.share_memory_\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    324\u001B[0m \u001B[38;5;129m@_share_memory_lock_protected\u001B[39m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshare_memory_\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshare_memory_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/storage.py:247\u001B[0m, in \u001B[0;36m_StorageBase.share_memory_\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# CUDA or PrivateUse1 doesn't use POSIX shared memory\u001B[39;00m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m get_sharing_strategy() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfile_system\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 247\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_share_filename_cpu_\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_share_fd_cpu_()\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/storage.py:297\u001B[0m, in \u001B[0;36m_share_memory_lock_protected.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    294\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 297\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    298\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;66;03m# If we acquired the storage lock here and we're done working on it\u001B[39;00m\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;66;03m# we can now release it and free the entry.\u001B[39;00m\n\u001B[1;32m    301\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m to_free \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    302\u001B[0m         \u001B[38;5;66;03m# Ensure that the cdata from the storage didn't change and only\u001B[39;00m\n\u001B[1;32m    303\u001B[0m         \u001B[38;5;66;03m# the data_ptr did.\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/storage.py:334\u001B[0m, in \u001B[0;36mUntypedStorage._share_filename_cpu_\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;129m@_share_memory_lock_protected\u001B[39m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_share_filename_cpu_\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 334\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_share_filename_cpu_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: no response from torch_shm_manager at \"/home/biscuitbobby/Documents/Projects/semantic_kernel_test/venv/lib64/python3.11/site-packages/torch/bin/torch_shm_manager\""
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize the custom LLM\n",
    "llm = CustomLLM(model_name=\"petals-team/StableBeluga2\")\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"\n",
    "Question: {question}\n",
    "Answer: Let's think step by step.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# Create the LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Generate a response\n",
    "question = \"What is the capital of France?\"\n",
    "response = llm_chain.run(question)\n",
    "print(response)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:31:49.563365520Z",
     "start_time": "2023-11-01T19:31:48.044194143Z"
    }
   },
   "id": "37354a53ffa60ef8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-01T17:49:27.224310051Z"
    }
   },
   "id": "fe43f75d50ae4ea0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
